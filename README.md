# Text-to-Music Generation using Stable Diffusion

This project focuses on generating music from text inputs utilizing stable diffusion techniques. The process involves converting textual descriptions into mel-spectrogram images, which are then used for latent space interpolation to achieve continuous music generation.

## Key Features:

- Utilizes stable diffusion methods for music generation from textual inputs.
- Converts text descriptions into mel-spectrogram images for further processing.
- Implements latent space interpolation techniques for continuous music generation.

## Dependencies:

- Python 3.x
- PyTorch
- Librosa - Mel-spectrogram processing
- Diffusers - HuggingFace

## Acknowledgments:

This project was inspired by the stable diffusion research and aims to provide an interesting approach to text-to-music generation.
